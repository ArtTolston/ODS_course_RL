{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be7a8b4-4156-4347-93f9-50bb1470e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "class SAC(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=1e-4, tau=0.2, \n",
    "                 batch_size=128, pi_lr=1e-3, q_lr=1e-3, period=15):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 2 * action_dim), nn.Tanh())\n",
    "\n",
    "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.period = period\n",
    "        self.counter = 0\n",
    "        self.step = 1\n",
    "\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
    "        self.q1_target_model = deepcopy(self.q1_model)\n",
    "        self.q2_target_model = deepcopy(self.q2_model)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, _, log_stds = self.predict_actions(state)\n",
    "        if self.step % 300 == 0:\n",
    "            print(f'std: {torch.exp(log_stds - 1.0).detach().data.numpy().reshape(-1)}')\n",
    "        return action.squeeze(1).detach().numpy()\n",
    "\n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "            rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
    "\n",
    "            next_actions, next_log_probs, _ = self.predict_actions(next_states)\n",
    "            next_states_and_actions = torch.concatenate((next_states, next_actions), dim=1)\n",
    "            next_q1_values = self.q1_target_model(next_states_and_actions)\n",
    "            next_q2_values = self.q2_target_model(next_states_and_actions)\n",
    "            next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "            targets = rewards + self.gamma * (1 - dones) * (next_min_q_values - self.alpha * next_log_probs)\n",
    "\n",
    "            states_and_actions = torch.concatenate((states, actions), dim=1)\n",
    "            q1_loss = torch.mean((self.q1_model(states_and_actions) - targets.detach()) ** 2)\n",
    "            q2_loss = torch.mean((self.q2_model(states_and_actions) - targets.detach()) ** 2)\n",
    "            self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
    "            self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
    "\n",
    "\n",
    "            pred_actions, log_probs, _ = self.predict_actions(states)\n",
    "            states_and_pred_actions = torch.concatenate((states, pred_actions), dim=1)\n",
    "            q1_values = self.q1_model(states_and_pred_actions)\n",
    "            q2_values = self.q2_model(states_and_pred_actions)\n",
    "            min_q_values = torch.min(q1_values, q2_values)\n",
    "            pi_loss = - torch.mean(min_q_values - self.alpha * log_probs)\n",
    "            self.update_model(pi_loss, self.pi_optimizer)\n",
    "\n",
    "            if self.step % 200 == 0:\n",
    "                print(f'q1 loss: {q1_loss.data.numpy()}')\n",
    "                print(f'q2 loss: {q2_loss.data.numpy()}')\n",
    "                print(f'pi loss: {pi_loss.data.numpy()}')\n",
    "\n",
    "            self.step += 1\n",
    "            \n",
    "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if model != None and target_model != None and self.counter < self.period:\n",
    "            for param, terget_param in zip(model.parameters(), target_model.parameters()):\n",
    "                new_terget_param = (1 - self.tau) * terget_param + self.tau * param\n",
    "                terget_param.data.copy_(new_terget_param)\n",
    "            self.counter = 0\n",
    "        self.counter += 1\n",
    "\n",
    "    def predict_actions(self, states):\n",
    "        means, log_stds = self.pi_model(states).T\n",
    "        means, log_stds = means.unsqueeze(1), log_stds.unsqueeze(1)\n",
    "        dists = Normal(means, torch.exp(log_stds - 1.0))\n",
    "        actions = dists.rsample()\n",
    "        log_probs = dists.log_prob(actions)\n",
    "        return actions, log_probs, log_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d17058b-597f-4432-a2ea-ea13b2b2899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/base/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/tmp/ipykernel_1346134/3922639635.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
      "/home/artem/base/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: -1172.3957731024402\n",
      "q1 loss: 95.40495300292969\n",
      "q2 loss: 50.970619201660156\n",
      "pi loss: 150.68511962890625\n",
      "total_reward: -1676.0291425053845\n",
      "std: [0.13534343]\n",
      "q1 loss: 69.11097717285156\n",
      "q2 loss: 45.59685516357422\n",
      "pi loss: 209.70877075195312\n",
      "total_reward: -1345.1315810761748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m     20\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m action)\n\u001b[0;32m---> 22\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     25\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m, in \u001b[0;36mSAC.fit\u001b[0;34m(self, state, action, reward, done, next_state)\u001b[0m\n\u001b[1;32m     74\u001b[0m states_and_pred_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcatenate((states, pred_actions), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m q1_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq1_model(states_and_pred_actions)\n\u001b[0;32m---> 76\u001b[0m q2_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_and_pred_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m min_q_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(q1_values, q2_values)\n\u001b[1;32m     78\u001b[0m pi_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(min_q_values \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m log_probs)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "totals = []\n",
    "for i in range(1):\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    agent = SAC(state_dim, action_dim)\n",
    "    \n",
    "    episode_n = 100\n",
    "    \n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "    \n",
    "        total_reward = 0\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        for t in range(200):\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(2 * action)\n",
    "        \n",
    "            agent.fit(state, action, reward, terminated or truncated, next_state)\n",
    "        \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'total_reward: {total_reward}')\n",
    "    \n",
    "    plt.plot(total_rewards)\n",
    "    plt.title('Total Rewards')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    totals.append(total_rewards)\n",
    "with open('SAC_Pendulum 3tr.json', 'w') as f:\n",
    "    json.dump(totals, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bc069a-7862-476c-a34e-8cd4cdcd3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Normal\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(), \n",
    "            nn.Linear(128, 128), nn.ReLU(), \n",
    "        )\n",
    "        self.mean_head = nn.Sequential(nn.Linear(128, action_dim), nn.Tanh())\n",
    "        self.std_head = nn.Sequential(\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Sigmoid()\n",
    "        ) \n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.body(input)\n",
    "        means = self.mean_head(input)\n",
    "        stds = self.std_head(input)\n",
    "        log_stds = torch.log(stds + 1e-10)\n",
    "        \n",
    "        return means, log_stds\n",
    "\n",
    "\n",
    "class SAC_many_dims(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=5e-1, tau=0.5, \n",
    "                 batch_size=128, pi_lr=1e-3, q_lr=2e-3, a_lr=1e-4, period=10, episode_n=500):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi_model = Actor(state_dim, action_dim)\n",
    "\n",
    "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.pi_model.name = \"policy\"\n",
    "        self.q1_model.name = \"q1\"\n",
    "        self.q2_model.name = \"q2\"\n",
    "\n",
    "        self.gamma = gamma\n",
    "        # self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.period = period\n",
    "        self.counter = 0\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.epoch_n = 2\n",
    "        self.step = 1\n",
    "        self.episode_n = episode_n\n",
    "        self.episode = 1\n",
    "\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
    "        self.pi_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.pi_optimizer, milestones=[int(0.4*episode_n), int(0.8*episode_n)], gamma=0.3)\n",
    "        self.q1_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.q1_optimizer, milestones=[int(0.4*episode_n), int(0.8*episode_n)], gamma=0.3)\n",
    "        self.q2_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.q2_optimizer, milestones=[int(0.4*episode_n), int(0.8*episode_n)], gamma=0.3)\n",
    "        self.q1_target_model = deepcopy(self.q1_model)\n",
    "        self.q2_target_model = deepcopy(self.q2_model)\n",
    "\n",
    "        self.target_entropy = 0.98 * -np.log(1 / self.action_dim)\n",
    "        self.log_alpha = torch.tensor(np.log(alpha), requires_grad=True)\n",
    "        self.alpha = self.log_alpha\n",
    "        self.alpha_optimiser = torch.optim.Adam([self.log_alpha], lr=a_lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, _, log_stds = self.predict_actions(state)\n",
    "        if self.step % 200 == 0:\n",
    "            print(f'std: {torch.exp(log_stds).detach().data.numpy().reshape(-1)}')\n",
    "            print(f'alpha: {round(self.alpha.detach().numpy().reshape(-1)[0] , 2)}')\n",
    "            self.step += 1\n",
    "        return action.squeeze(0).detach().numpy()\n",
    "\n",
    "    def add_five(self, state, action, reward, done, next_state):\n",
    "        if done:\n",
    "            self.episode += 1\n",
    "            if self.episode == 50 or self.episode == 150 or self.episode == 300:\n",
    "                self.alpha *= 0.4\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "    def fit(self):\n",
    "        mean_pi_loss = []\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            for i in range(self.epoch_n):\n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "                rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
    "    \n",
    "                next_actions, next_log_probs, _ = self.predict_actions(next_states)\n",
    "                next_states_and_actions = torch.concatenate((next_states, next_actions), dim=1)\n",
    "                next_q1_values = self.q1_target_model(next_states_and_actions)\n",
    "                next_q2_values = self.q2_target_model(next_states_and_actions)\n",
    "                next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "                targets = rewards + self.gamma * (1 - dones) * (next_min_q_values - self.alpha * torch.sum(next_log_probs, dim=1).unsqueeze(1))\n",
    "                # print(f'targets {targets}')\n",
    "    \n",
    "                states_and_actions = torch.concatenate((states, actions), dim=1)\n",
    "                # print(f'st_and_act {states_and_actions.shape}')\n",
    "                q1_loss = torch.mean((self.q1_model(states_and_actions) - targets.detach()) ** 2)\n",
    "                q2_loss = torch.mean((self.q2_model(states_and_actions) - targets.detach()) ** 2)\n",
    "                # print(f'q1_loss {q1_loss.shape}')\n",
    "                self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
    "                self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
    "\n",
    "                self.q1_scheduler.step()\n",
    "                self.q2_scheduler.step()\n",
    "\n",
    "                if self.step % 100 == 0:\n",
    "                    print(f'q1 loss: {q1_loss.data.numpy()}')\n",
    "                    print(f'q2 loss: {q2_loss.data.numpy()}')\n",
    "\n",
    "                if i % 2 == 0:\n",
    "                    pred_actions, log_probs, log_stds = self.predict_actions(states)\n",
    "                    # print(pred_actions.requires_grad)\n",
    "                    # print(f'pred_act {pred_actions.shape}')\n",
    "                    # print(f'log_probs {log_probs.shape}')\n",
    "                    # print(f'log_stds {log_stds.shape}')\n",
    "                    states_and_pred_actions = torch.concatenate((states, pred_actions), dim=1)\n",
    "\n",
    "                    self.disable_grads(self.q1_model)\n",
    "                    self.disable_grads(self.q2_model)\n",
    "                    \n",
    "                    q1_values = self.q1_model(states_and_pred_actions)\n",
    "                    q2_values = self.q2_model(states_and_pred_actions)\n",
    "                    min_q_values = torch.min(q1_values, q2_values)\n",
    "                    pi_loss = - torch.mean(min_q_values + self.alpha * torch.sum(log_stds, dim=1).unsqueeze(1))\n",
    "                    \n",
    "                    if self.step % 100 == 0:\n",
    "                        print(f'pi loss: {pi_loss.data.numpy()}')\n",
    "                        self.step += 1\n",
    "                    \n",
    "                    mean_pi_loss.append(pi_loss.data.numpy())\n",
    "                    self.update_model(pi_loss, self.pi_optimizer, self.pi_model)\n",
    "\n",
    "                    self.pi_scheduler.step()\n",
    "\n",
    "                    self.enable_grads(self.q1_model)\n",
    "                    self.enable_grads(self.q2_model)\n",
    "\n",
    "                    alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "                    alpha_loss.backward()\n",
    "                    self.alpha_optimiser.step()\n",
    "                    self.alpha = self.log_alpha.exp()\n",
    "                    self.alpha_optimiser.zero_grad()\n",
    "                    \n",
    "                self.counter += 1\n",
    "            \n",
    "            self.step += 1\n",
    "                \n",
    "            return np.mean(mean_pi_loss)\n",
    "        else:\n",
    "            return 0\n",
    "            \n",
    "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
    "        loss.backward()\n",
    "        if model.name == \"policy\":\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "        else:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if model != None and target_model != None and self.counter % self.period == 0:\n",
    "            for param, terget_param in zip(model.parameters(), target_model.parameters()):\n",
    "                new_terget_param = (1 - self.tau) * terget_param + self.tau * param\n",
    "                terget_param.data.copy_(new_terget_param)\n",
    "\n",
    "    def predict_actions(self, states):\n",
    "        means, log_stds = self.pi_model(states)\n",
    "        # вычитаем единицу чтобы дать возможность больше сузить ДИ при желании нейросети\n",
    "        dists = Normal(means, torch.exp(log_stds))\n",
    "        actions = dists.rsample()\n",
    "        actions = torch.tanh(actions)\n",
    "        log_probs = dists.log_prob(actions)\n",
    "        return actions, log_probs, log_stds\n",
    "\n",
    "    def disable_grads(self, net):\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def enable_grads(self, net):\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ac4d70-b991-445d-9bbb-e6ee9b6ebcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SAC(alpha, lr):\n",
    "    env = gym.make('LunarLander-v2', continuous=True)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    episode_n = 1000\n",
    "\n",
    "    agent = SAC_many_dims(state_dim, action_dim, episode_n=episode_n)\n",
    "    \n",
    "    mean_pi_loss = []\n",
    "    \n",
    "    total_rewards = []\n",
    "    counter = 0\n",
    "    for episode in range(episode_n):\n",
    "    \n",
    "        total_reward = 0\n",
    "        state, info = env.reset()\n",
    "\n",
    "        pi_loss = []\n",
    "        \n",
    "        for i in range(700):\n",
    "            action = agent.get_action(state)\n",
    "    \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            agent.add_five(state, action, reward, terminated or truncated, next_state)\n",
    "            counter += 1\n",
    "\n",
    "            if counter % 4 == 0:\n",
    "                _pi_loss = agent.fit()\n",
    "                pi_loss.append(_pi_loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        if episode % 3 == 0:\n",
    "            print(f'episode: {episode},  mean:  {np.mean(total_rewards[-3:])}')\n",
    "        total_rewards.append(total_reward)\n",
    "        mean_pi_loss.append(np.mean(pi_loss))\n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.plot(total_rewards)\n",
    "    plt.title(f'Total Rewards SAC alpha={alpha}, lr={lr}')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.plot(mean_pi_loss)\n",
    "    plt.title(f'Mean policy loss SAC alpha={alpha}, lr={lr}')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6616d2a-52a5-478e-bc61-50ac2ce7c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# начался рост. дошло до 110 пунктов в лунарлэндере.\n",
    "# нормальные значения для лосса политики - чем меньше тем лучше. В данном случае дошло примерно до -20.\n",
    "# нормальные значения для лосса критика - чем ближе к нулю тем лучше. Дошло до 0.5 что есть хорошо.\n",
    "# использовалась фиксированная альфа равная 0.1. \n",
    "# gamma=0.99, alpha=1e-1, tau=0.5, \n",
    "# batch_size=64, pi_lr=1e-3, q_lr=1e-3, a_lr=1e-4, period=15\n",
    "# от этих значений уже будем плясать\n",
    "\n",
    "# первый вариант улучшения - постепенно понижать альфу + увеличить количество итераций чтобы можно было сойтись.\n",
    "# Если не сойдется то можно добавить lr decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a307b0-19b6-4f55-8fc7-2d5e606bcac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/base/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/artem/base/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0,  mean:  nan\n",
      "episode: 3,  mean:  -299.9676894669637\n",
      "q1 loss: 14.98336410522461\n",
      "q2 loss: 11.565777778625488\n",
      "pi loss: 20.202425003051758\n",
      "episode: 6,  mean:  -521.6188957450012\n",
      "std: [0.95858806 0.84380144]\n",
      "alpha: 0.49\n",
      "episode: 9,  mean:  -68.49529716661343\n",
      "q1 loss: 8.335103988647461\n",
      "q2 loss: 9.504855155944824\n",
      "pi loss: 17.073970794677734\n",
      "std: [0.5846963  0.39073575]\n",
      "alpha: 0.48\n",
      "q1 loss: 130.7303009033203\n",
      "q2 loss: 141.00413513183594\n",
      "pi loss: 1.6977260112762451\n",
      "episode: 12,  mean:  -121.76061326766832\n",
      "std: [0.7597721  0.58224356]\n",
      "alpha: 0.47\n",
      "q1 loss: 40.67259216308594\n",
      "q2 loss: 39.667945861816406\n",
      "pi loss: 2.453193426132202\n",
      "std: [0.6007917 0.4121177]\n",
      "alpha: 0.46\n",
      "episode: 15,  mean:  31.88972525547288\n",
      "q1 loss: 315.13592529296875\n",
      "q2 loss: 342.73541259765625\n",
      "pi loss: -7.831305027008057\n",
      "std: [0.7417977 0.6044586]\n",
      "alpha: 0.45\n",
      "q1 loss: 10.437310218811035\n",
      "q2 loss: 11.839136123657227\n",
      "pi loss: -13.07581615447998\n",
      "std: [0.79745543 0.714872  ]\n",
      "alpha: 0.44\n",
      "episode: 18,  mean:  -7.505905560442375\n",
      "q1 loss: 41.77987289428711\n",
      "q2 loss: 45.89520263671875\n",
      "pi loss: -22.28256607055664\n",
      "std: [0.68191403 0.5999244 ]\n",
      "alpha: 0.44\n",
      "q1 loss: 7.191286087036133\n",
      "q2 loss: 6.208192825317383\n",
      "pi loss: -25.713661193847656\n",
      "std: [0.6611104 0.6253929]\n",
      "alpha: 0.43\n",
      "q1 loss: 6.968417167663574\n",
      "q2 loss: 5.9102678298950195\n",
      "pi loss: -23.57040786743164\n",
      "std: [0.67530686 0.627139  ]\n",
      "alpha: 0.42\n",
      "episode: 21,  mean:  10.698392045690381\n",
      "q1 loss: 48.13090515136719\n",
      "q2 loss: 42.393760681152344\n",
      "pi loss: -32.418479919433594\n",
      "std: [0.6639803 0.6540804]\n",
      "alpha: 0.41\n",
      "q1 loss: 31.07619285583496\n",
      "q2 loss: 28.960811614990234\n",
      "pi loss: -47.689205169677734\n",
      "std: [0.85937256 0.8867427 ]\n",
      "alpha: 0.4\n",
      "q1 loss: 2.664703369140625\n",
      "q2 loss: 2.7595977783203125\n",
      "pi loss: -49.1903190612793\n",
      "episode: 24,  mean:  -14.147730156129422\n",
      "std: [0.82426304 0.86375356]\n",
      "alpha: 0.4\n",
      "q1 loss: 27.96798324584961\n",
      "q2 loss: 21.636554718017578\n",
      "pi loss: -46.40765380859375\n",
      "std: [0.82653886 0.85066456]\n",
      "alpha: 0.39\n",
      "q1 loss: 64.18026733398438\n",
      "q2 loss: 58.793212890625\n",
      "pi loss: -46.678009033203125\n",
      "std: [0.8652986  0.86447084]\n",
      "alpha: 0.38\n",
      "episode: 27,  mean:  19.426590917329033\n",
      "q1 loss: 3.2854719161987305\n",
      "q2 loss: 3.3706541061401367\n",
      "pi loss: -53.2734489440918\n",
      "std: [0.8903521  0.89226294]\n",
      "alpha: 0.38\n",
      "q1 loss: 3.3015284538269043\n",
      "q2 loss: 2.8872179985046387\n",
      "pi loss: -51.46282196044922\n",
      "std: [0.8852757  0.87165356]\n",
      "alpha: 0.37\n",
      "q1 loss: 5.049019813537598\n",
      "q2 loss: 5.636114120483398\n",
      "pi loss: -54.35900115966797\n",
      "std: [0.84849495 0.8240048 ]\n",
      "alpha: 0.36\n",
      "episode: 30,  mean:  4.4594806945359915\n",
      "q1 loss: 2.3689498901367188\n",
      "q2 loss: 2.5821382999420166\n",
      "pi loss: -56.0538330078125\n",
      "std: [0.97223026 0.9834012 ]\n",
      "alpha: 0.35\n",
      "q1 loss: 11.173066139221191\n",
      "q2 loss: 20.602466583251953\n",
      "pi loss: -52.97543716430664\n",
      "std: [0.95480615 0.96452326]\n",
      "alpha: 0.35\n",
      "q1 loss: 3.367001533508301\n",
      "q2 loss: 3.7638540267944336\n",
      "pi loss: -53.17695236206055\n",
      "episode: 33,  mean:  23.846110818346336\n",
      "std: [0.9149299 0.9529412]\n",
      "alpha: 0.34\n",
      "q1 loss: 68.35243225097656\n",
      "q2 loss: 76.07302856445312\n",
      "pi loss: -54.23540496826172\n",
      "std: [0.9273065 0.9648343]\n",
      "alpha: 0.33\n",
      "q1 loss: 2.11647891998291\n",
      "q2 loss: 1.7587801218032837\n",
      "pi loss: -54.301658630371094\n",
      "std: [0.9180624 0.948125 ]\n",
      "alpha: 0.33\n",
      "episode: 36,  mean:  16.174587068665222\n",
      "q1 loss: 2.724705457687378\n",
      "q2 loss: 2.4205777645111084\n",
      "pi loss: -56.92300033569336\n",
      "std: [0.9278464  0.95190775]\n",
      "alpha: 0.32\n",
      "q1 loss: 3.40859055519104\n",
      "q2 loss: 2.385103464126587\n",
      "pi loss: -56.38594436645508\n",
      "std: [0.91775054 0.9544909 ]\n",
      "alpha: 0.31\n",
      "q1 loss: 2.8162593841552734\n",
      "q2 loss: 4.74744176864624\n",
      "pi loss: -55.38885498046875\n",
      "std: [0.9220659 0.9749654]\n",
      "alpha: 0.31\n",
      "episode: 39,  mean:  7.667092245207051\n",
      "q1 loss: 7.415331840515137\n",
      "q2 loss: 7.793274402618408\n",
      "pi loss: -52.076263427734375\n",
      "std: [0.97739613 0.99848396]\n",
      "alpha: 0.3\n",
      "q1 loss: 2.1019887924194336\n",
      "q2 loss: 1.9682801961898804\n",
      "pi loss: -59.598411560058594\n",
      "std: [0.95716405 0.99693847]\n",
      "alpha: 0.3\n",
      "q1 loss: 2.6223108768463135\n",
      "q2 loss: 2.2273175716400146\n",
      "pi loss: -53.04036331176758\n",
      "episode: 42,  mean:  8.86362595590657\n",
      "std: [0.94043005 0.9934496 ]\n",
      "alpha: 0.29\n",
      "q1 loss: 2.3036959171295166\n",
      "q2 loss: 1.8507440090179443\n",
      "pi loss: -51.56665802001953\n",
      "std: [0.9561391 0.9931242]\n",
      "alpha: 0.29\n",
      "q1 loss: 2.2364437580108643\n",
      "q2 loss: 2.153974771499634\n",
      "pi loss: -53.69963836669922\n",
      "std: [0.94322294 0.98906505]\n",
      "alpha: 0.28\n",
      "episode: 45,  mean:  1.7220024590476328\n",
      "q1 loss: 3.340313196182251\n",
      "q2 loss: 8.077764511108398\n",
      "pi loss: -40.12682342529297\n",
      "std: [0.9048891 0.9826423]\n",
      "alpha: 0.27\n",
      "q1 loss: 2.36148738861084\n",
      "q2 loss: 2.387756586074829\n",
      "pi loss: -51.90597152709961\n",
      "std: [0.9401888  0.99064016]\n",
      "alpha: 0.27\n",
      "q1 loss: 277.63763427734375\n",
      "q2 loss: 277.8048400878906\n",
      "pi loss: -50.231502532958984\n",
      "episode: 48,  mean:  10.610448430786482\n",
      "std: [0.8951423  0.99066836]\n",
      "alpha: 0.26\n",
      "q1 loss: 2.01213002204895\n",
      "q2 loss: 1.910690188407898\n",
      "pi loss: -50.09780502319336\n",
      "std: [0.9315618 0.9966575]\n",
      "alpha: 0.26\n",
      "q1 loss: 3.1406335830688477\n",
      "q2 loss: 2.6052050590515137\n",
      "pi loss: -50.312095642089844\n",
      "std: [0.9414398 0.9957035]\n",
      "alpha: 0.25\n",
      "q1 loss: 2.1623754501342773\n",
      "q2 loss: 1.5782725811004639\n",
      "pi loss: -50.979087829589844\n",
      "episode: 51,  mean:  9.755732816200975\n",
      "std: [0.92086524 0.99305224]\n",
      "alpha: 0.25\n",
      "q1 loss: 1.5077567100524902\n",
      "q2 loss: 1.3184233903884888\n",
      "pi loss: -47.72085952758789\n",
      "std: [0.9350288 0.9903645]\n",
      "alpha: 0.24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m lrs:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtest_SAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mtest_SAC\u001b[0;34m(alpha, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m700\u001b[39m):\n\u001b[1;32m     22\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m---> 24\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     agent\u001b[38;5;241m.\u001b[39madd_five(state, action, reward, terminated \u001b[38;5;129;01mor\u001b[39;00m truncated, next_state)\n\u001b[1;32m     27\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:556\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    545\u001b[0m     p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    546\u001b[0m         (ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, oy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    547\u001b[0m         impulse_pos,\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    551\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    552\u001b[0m         impulse_pos,\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    554\u001b[0m     )\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    559\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:59\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     56\u001b[0m     contactListener\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureA\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureB\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     63\u001b[0m     ):\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "lrs = [1e-3]\n",
    "alpha = 1e-1\n",
    "for lr in lrs:\n",
    "    test_SAC(alpha, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "662b5db4-f299-4d58-8bf5-d005440add6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 2 * 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fc06875-4ff6-467c-9ed7-1cdfb78b93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SAC_discrete(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=1e-4, tau=0.2, \n",
    "                 batch_size=128, pi_lr=1e-3, q_lr=1e-3, period=15):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, action_dim), nn.Softmax())\n",
    "\n",
    "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 128), nn.ReLU(), \n",
    "                                      nn.Linear(128, 1))\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.period = period\n",
    "        self.counter = 0\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.epoch_n = 20\n",
    "        \n",
    "\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
    "        self.q1_target_model = deepcopy(self.q1_model)\n",
    "        self.q2_target_model = deepcopy(self.q2_model)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, _ = self.predict_actions(state)\n",
    "        return action.squeeze(0).detach().numpy()\n",
    "\n",
    "    def add_five(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, done, next_state])\n",
    "\n",
    "    def fit(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            for _ in range(self.epoch_n):\n",
    "                batch = random.sample(self.memory, self.batch_size)\n",
    "                states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
    "                rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
    "    \n",
    "                next_actions, next_log_probs = self.predict_actions(next_states)\n",
    "                next_states_and_actions = torch.concatenate((next_states, next_actions), dim=1)\n",
    "                next_q1_values = self.q1_target_model(next_states_and_actions)\n",
    "                next_q2_values = self.q2_target_model(next_states_and_actions)\n",
    "                next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "                targets = rewards + self.gamma * (1 - dones) * (next_min_q_values - self.alpha * next_log_probs)\n",
    "    \n",
    "                states_and_actions = torch.concatenate((states, actions), dim=1)\n",
    "                q1_loss = torch.mean((self.q1_model(states_and_actions) - targets.detach()) ** 2)\n",
    "                q2_loss = torch.mean((self.q2_model(states_and_actions) - targets.detach()) ** 2)\n",
    "                self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
    "                self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
    "    \n",
    "                pred_actions, log_probs = self.predict_actions(states)\n",
    "                states_and_pred_actions = torch.concatenate((states, pred_actions), dim=1)\n",
    "                q1_values = self.q1_model(states_and_pred_actions)\n",
    "                q2_values = self.q2_model(states_and_pred_actions)\n",
    "                min_q_values = torch.min(q1_values, q2_values)\n",
    "                pi_loss = - torch.mean(min_q_values - self.alpha * log_probs)\n",
    "                self.update_model(pi_loss, self.pi_optimizer)\n",
    "            \n",
    "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if model != None and target_model != None and self.counter < self.period:\n",
    "            for param, terget_param in zip(model.parameters(), target_model.parameters()):\n",
    "                new_terget_param = (1 - self.tau) * terget_param + self.tau * param\n",
    "                terget_param.data.copy_(new_terget_param)\n",
    "            self.counter = 0\n",
    "        self.counter += 1\n",
    "\n",
    "    def predict_actions(self, states):\n",
    "        probs = self.pi_model(states)\n",
    "        print(probs)\n",
    "        dists = Categorical(probs=probs)\n",
    "        print(dists)\n",
    "        actions = dists.sample()\n",
    "        print(actions)\n",
    "        log_probs = dists.log_prob(actions)\n",
    "        return actions, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f945823-1ce8-4130-aebc-312c316bbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = SAC_discrete(state_dim, action_dim)\n",
    "\n",
    "episode_n = 1\n",
    "\n",
    "total_rewards = []\n",
    "for episode in range(episode_n):\n",
    "\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(700):\n",
    "        action = agent.get_action(state)\n",
    "        print(action)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        agent.add_five(state, action, reward, done, next_state)\n",
    "    \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    agent.fit()\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "plt.plot(total_rewards)\n",
    "plt.title('Total Rewards')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
