{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6deb587-1d87-46c3-a5e8-9ca4dccbf86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/base/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e340f9-8cd6-4ea4-817c-33681a798dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a965c8d-619c-4cd8-a1dd-ff87a7252662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskVelocityWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Gym environment observation wrapper used to mask velocity terms in\n",
    "    observations. The intention is the make the MDP partially observatiable.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(MaskVelocityWrapper, self).__init__(env)\n",
    "        if ENV == \"CartPole-v1\":\n",
    "            self.mask = np.array([1., 0., 1., 0.])\n",
    "        elif ENV == \"Pendulum-v0\":\n",
    "            self.mask = np.array([1., 1., 0.])\n",
    "        elif ENV == \"LunarLander-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        elif ENV == \"LunarLanderContinuous-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return  observation * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858b59fa-db10-45ed-8563-ca04c79807b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, action_dim)                \n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        actions = self.model(states)\n",
    "        return actions\n",
    "\n",
    "class DuelingQfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU()           \n",
    "        )\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.a_head = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        inp = self.model(states)\n",
    "        v = self.v_head(inp)\n",
    "        a = self.a_head(inp)\n",
    "\n",
    "        q = v + a - a.mean(1, keepdim=True)\n",
    "        return q\n",
    "\n",
    "class DuelingQfunctionWithEncoder(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU()           \n",
    "        )\n",
    "        self.v_head = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.a_head = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        \n",
    "        inp = self.model(states)\n",
    "        v = self.v_head(inp)\n",
    "        a = self.a_head(inp)\n",
    "\n",
    "        q = v + a - a.mean(1, keepdim=True)\n",
    "        return q\n",
    "\n",
    "\n",
    "class DQN_double:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.0001, epsilon_min=0.0001, period=100):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = DuelingQfunction(state_dim, action_dim).to(device)\n",
    "        self.freezing_q_function = DuelingQfunction(self.state_dim, self.action_dim).to(device)\n",
    "        # self.q_function = Qfunction(state_dim, action_dim).to(device)\n",
    "        # self.freezing_q_function = Qfunction(self.state_dim, self.action_dim).to(device)\n",
    "        # self.q_function = QNetwork(state_dim, action_dim).to(device)\n",
    "        # self.freezing_q_function = QNetwork(self.state_dim, self.action_dim).to(device)\n",
    "        # self.freezing_q_function.load_state_dict(self.q_function.state_dict())\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 0.5\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = []\n",
    "        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.counter = 1\n",
    "        self.period = period\n",
    "\n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_function(torch.FloatTensor(state).unsqueeze(dim=0).to(device))\n",
    "            q_values = q_values.squeeze()\n",
    "            \n",
    "            argmax_action = torch.argmax(q_values)\n",
    "            probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "            probs[argmax_action] += 1.0 - self.epsilon\n",
    "            if self.counter % 10_000 == 0:\n",
    "                print(probs)\n",
    "                print(self.epsilon)\n",
    "                print(q_values)\n",
    "            action = np.random.choice(np.arange(self.action_dim), p=probs)\n",
    "            return action\n",
    "\n",
    "    def add(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "    \n",
    "    def fit(self):\n",
    "        if len(self.memory) > 128:\n",
    "\n",
    "            if self.counter % self.period == 0:\n",
    "                for parameter_freeze, parameter in zip(self.freezing_q_function.model.parameters(), self.q_function.model.parameters()):\n",
    "                    with torch.no_grad():\n",
    "                        parameter_freeze.data.copy_(parameter.data)\n",
    "                for parameter_freeze, parameter in zip(self.freezing_q_function.a_head.parameters(), self.q_function.a_head.parameters()):\n",
    "                    with torch.no_grad():\n",
    "                        parameter_freeze.data.copy_(parameter.data)\n",
    "                for parameter_freeze, parameter in zip(self.freezing_q_function.v_head.parameters(), self.q_function.v_head.parameters()):\n",
    "                    with torch.no_grad():\n",
    "                        parameter_freeze.data.copy_(parameter.data)\n",
    "                # self.freezing_q_function.load_state_dict(self.q_function.state_dict())\n",
    "            \n",
    "            self.counter += 1\n",
    "\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, list(zip(*batch)))\n",
    "            states, actions, rewards, dones, next_states = states.to(device), actions.unsqueeze(1).to(device), rewards.unsqueeze(1).to(device), dones.unsqueeze(1).to(device), next_states.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                targets = rewards + self.gamma * (1 - dones) * self.freezing_q_function(next_states.float()).gather(1, torch.argmax(self.q_function(next_states.float()), dim=1, keepdim=True).long())\n",
    "\n",
    "            loss = F.mse_loss(self.q_function(states.float()).gather(1, actions.long()), targets)\n",
    "\n",
    "            self.optimzaer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimzaer.step()\n",
    "            if self.counter % 10_000 == 0:\n",
    "                print(f\"loss: {loss.item()}\")\n",
    "                # print(targets)\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= (0.1 - self.epsilon_min) / 20_000\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d675650-917c-4424-8dfd-b8f9788fd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_learning(env, agent, episode_n = 100, t_max = 500, N=2):\n",
    "    # agent.epsilon_decrease = 1.0 / (0.75*episode_n)\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "        total_reward = 0\n",
    "    \n",
    "        state, info = env.reset()\n",
    "        \n",
    "        for t in range(1, t_max):\n",
    "            \n",
    "            start_state = None\n",
    "            start_action = None\n",
    "            next_state = None\n",
    "            done = False\n",
    "            cum_reward = 0\n",
    "            \n",
    "            for i in range(N):\n",
    "                action = agent.get_action(state)\n",
    "                if i == 0:\n",
    "                    start_state = state\n",
    "                    start_action = action\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                cum_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "                    break\n",
    "    \n",
    "            total_reward += cum_reward\n",
    "            \n",
    "            agent.add(start_state, start_action, cum_reward, done, next_state)\n",
    "\n",
    "            agent.fit()\n",
    "            \n",
    "            state = next_state\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(total_reward)\n",
    "        if episode % 10 == 0:\n",
    "            print(f'episode: {episode}, total_reward: {np.mean(total_rewards[-10:])}')\n",
    "    \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea48f957-bde3-48e3-b779-ae9bebc5fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: -642.5320195837588\n",
      "episode: 10, total_reward: -232.2707062217134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m episode_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN_double(state_dim, action_dim)\n\u001b[0;32m---> 13\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mDQN_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mDQN_learning\u001b[0;34m(env, agent, episode_n, t_max, N)\u001b[0m\n\u001b[1;32m     33\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cum_reward\n\u001b[1;32m     35\u001b[0m agent\u001b[38;5;241m.\u001b[39madd(start_state, start_action, cum_reward, done, next_state)\n\u001b[0;32m---> 37\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[4], line 120\u001b[0m, in \u001b[0;36mDQN_double.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m     targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreezing_q_function(next_states\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, torch\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_function(next_states\u001b[38;5;241m.\u001b[39mfloat()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_function(states\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39mlong()), targets)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimzaer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimzaer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/_dynamo/decorators.py:46\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     44\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:437\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:823\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m all_bytecode_suffixes):\n\u001b[1;32m    821\u001b[0m     filename \u001b[38;5;241m=\u001b[39m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    822\u001b[0m                 importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mSOURCE_SUFFIXES[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    824\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "episode_n=500\n",
    "\n",
    "agent = DQN_double(state_dim, action_dim)\n",
    "\n",
    "total_rewards = DQN_learning(env, agent, episode_n=episode_n, N=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab1cb97-479e-489c-bb51-a37577116fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: 16.0\n",
      "episode: 10, total_reward: 23.7\n",
      "episode: 20, total_reward: 17.1\n",
      "episode: 30, total_reward: 14.6\n",
      "episode: 40, total_reward: 19.4\n",
      "episode: 50, total_reward: 16.5\n",
      "episode: 60, total_reward: 24.5\n",
      "episode: 70, total_reward: 32.7\n",
      "episode: 80, total_reward: 27.4\n",
      "episode: 90, total_reward: 21.1\n",
      "episode: 100, total_reward: 17.0\n",
      "episode: 110, total_reward: 41.3\n",
      "episode: 120, total_reward: 29.8\n",
      "episode: 130, total_reward: 41.9\n",
      "episode: 140, total_reward: 44.1\n",
      "episode: 150, total_reward: 45.6\n",
      "episode: 160, total_reward: 30.9\n",
      "episode: 170, total_reward: 35.6\n",
      "episode: 180, total_reward: 40.9\n",
      "episode: 190, total_reward: 45.5\n",
      "episode: 200, total_reward: 42.6\n",
      "episode: 210, total_reward: 33.2\n",
      "episode: 220, total_reward: 32.5\n",
      "episode: 230, total_reward: 41.3\n",
      "episode: 240, total_reward: 42.6\n",
      "episode: 250, total_reward: 34.6\n",
      "episode: 260, total_reward: 31.3\n",
      "episode: 270, total_reward: 45.8\n",
      "episode: 280, total_reward: 39.1\n",
      "episode: 290, total_reward: 38.1\n",
      "episode: 300, total_reward: 37.2\n",
      "episode: 310, total_reward: 39.2\n",
      "episode: 320, total_reward: 36.2\n",
      "episode: 330, total_reward: 36.5\n",
      "episode: 340, total_reward: 38.5\n",
      "episode: 350, total_reward: 46.6\n",
      "episode: 360, total_reward: 40.7\n",
      "episode: 370, total_reward: 48.3\n",
      "episode: 380, total_reward: 39.6\n",
      "episode: 390, total_reward: 32.9\n",
      "episode: 400, total_reward: 43.0\n",
      "episode: 410, total_reward: 48.1\n",
      "episode: 420, total_reward: 27.3\n",
      "episode: 430, total_reward: 42.1\n",
      "episode: 440, total_reward: 43.4\n",
      "episode: 450, total_reward: 45.4\n",
      "episode: 460, total_reward: 34.0\n",
      "episode: 470, total_reward: 36.5\n",
      "episode: 480, total_reward: 40.5\n",
      "episode: 490, total_reward: 40.7\n",
      "episode: 500, total_reward: 48.6\n",
      "episode: 510, total_reward: 36.0\n",
      "episode: 520, total_reward: 42.0\n",
      "episode: 530, total_reward: 40.1\n",
      "episode: 540, total_reward: 47.4\n",
      "episode: 550, total_reward: 46.3\n",
      "episode: 560, total_reward: 58.7\n",
      "episode: 570, total_reward: 38.1\n",
      "episode: 580, total_reward: 43.9\n",
      "episode: 590, total_reward: 51.3\n",
      "episode: 600, total_reward: 36.8\n",
      "episode: 610, total_reward: 32.8\n",
      "episode: 620, total_reward: 43.9\n",
      "episode: 630, total_reward: 49.5\n",
      "episode: 640, total_reward: 41.4\n",
      "episode: 650, total_reward: 31.8\n",
      "episode: 660, total_reward: 45.8\n",
      "episode: 670, total_reward: 39.2\n",
      "episode: 680, total_reward: 36.2\n",
      "episode: 690, total_reward: 43.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m episode_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     12\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN_double(state_dim, action_dim)\n\u001b[0;32m---> 14\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mDQN_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mDQN_learning\u001b[0;34m(env, agent, episode_n, t_max, N)\u001b[0m\n\u001b[1;32m     33\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cum_reward\n\u001b[1;32m     35\u001b[0m agent\u001b[38;5;241m.\u001b[39madd(start_state, start_action, cum_reward, done, next_state)\n\u001b[0;32m---> 37\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn[4], line 122\u001b[0m, in \u001b[0;36mDQN_double.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimzaer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    121\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimzaer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10_000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "torch.manual_seed(43)\n",
    "ENV = \"CartPole-v1\"\n",
    "env = gym.make(ENV)\n",
    "env = MaskVelocityWrapper(env)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "episode_n=1000\n",
    "\n",
    "agent = DQN_double(state_dim, action_dim)\n",
    "\n",
    "total_rewards = DQN_learning(env, agent, episode_n=episode_n, N=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
