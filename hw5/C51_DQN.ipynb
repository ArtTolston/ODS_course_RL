{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "890dd138-a5db-4a95-926c-16bce66fedcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/base/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e565e0c2-d0c2-48ef-8f8d-f1380aa5b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a171eb6a-8cf6-4b56-989c-743b8f38a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_atoms):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, n_atoms),\n",
    "                nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        concat_input = torch.cat((states, actions), dim=1)\n",
    "        reward_distr = self.model(concat_input)\n",
    "        return reward_distr\n",
    "\n",
    "\n",
    "class DQN_double:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 gamma=0.95,\n",
    "                 lr=1e-3,\n",
    "                 batch_size=64,\n",
    "                 epsilon_decrease=0.002,\n",
    "                 epsilon_min=0.0001,\n",
    "                 period=100,\n",
    "                 n_atoms=51,\n",
    "                 v_min=-10,\n",
    "                 v_max=10\n",
    "                ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = Qfunction(state_dim, action_dim, n_atoms).to(device)\n",
    "        self.freezing_q_function = Qfunction(state_dim, action_dim, n_atoms).to(device)\n",
    "        self.freezing_q_function.load_state_dict(self.q_function.state_dict())\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = []\n",
    "        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.counter = 1\n",
    "        self.period = period\n",
    "        \n",
    "        self.n_atoms = n_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atoms = self.get_atoms()\n",
    "\n",
    "    def get_atoms(self):\n",
    "        atoms = np.zeros((self.n_atoms,))\n",
    "        for i in range(self.n_atoms):\n",
    "            atoms[i] = self.v_min + i * (self.v_max - self.v_min) / (self.n_atoms - 1.0)\n",
    "        return atoms\n",
    "\n",
    "\n",
    "    def get_action(self, obs, show=False):\n",
    "        with torch.no_grad():\n",
    "            one_hot_action = np.zeros((self.action_dim,))\n",
    "            max_action = 0\n",
    "            max_q_value = -np.inf\n",
    "            for i in range(self.action_dim):\n",
    "                one_hot_action[i] = 1\n",
    "                state = torch.FloatTensor(obs).unsqueeze(dim=0).to(device)\n",
    "                action = torch.FloatTensor(one_hot_action).unsqueeze(dim=0).to(device)\n",
    "                # print(f\"state: {state.shape}\")\n",
    "                # print(f\"action: {action.shape}\")\n",
    "                reward_distr = self.q_function(state, action)\n",
    "                reward_distr = reward_distr.squeeze().cpu().numpy()\n",
    "                q_value = np.sum(self.atoms * reward_distr)\n",
    "                if show:\n",
    "                    print(f\"state: {obs}\")\n",
    "                    print(f\"action: {i}\")\n",
    "                    print(f\"q_value: {q_value}\")\n",
    "                if q_value > max_q_value:\n",
    "                    max_q_value = q_value\n",
    "                    max_action = i\n",
    "                one_hot_action[i] = 0\n",
    "                \n",
    "            probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "            probs[max_action] += 1.0 - self.epsilon\n",
    "            if show:\n",
    "                print(f\"probs: {probs}\")\n",
    "            if self.counter % 500 == 0:\n",
    "                print(probs)\n",
    "            action = np.random.choice(np.arange(self.action_dim), p=probs)\n",
    "            return action\n",
    "\n",
    "    def project(self, values, distr):\n",
    "        j = 0\n",
    "        new_target_distr = np.zeros((self.n_atoms, ))\n",
    "        for i in range(self.n_atoms):\n",
    "            if values[i] <= self.atoms[0]:\n",
    "                new_target_distr[0] += distr[i]\n",
    "            elif values[i] >= self.atoms[-1]:\n",
    "                new_target_distr[-1] += distr[i]\n",
    "            else:\n",
    "                while values[i] > self.atoms[j]:\n",
    "                    j += 1\n",
    "                new_target_distr[j - 1] += distr[i] * (self.atoms[j] - values[i]) / (self.atoms[j] - self.atoms[j - 1]) \n",
    "                new_target_distr[j] += distr[i] * (values[i] - self.atoms[j - 1]) / (self.atoms[j] - self.atoms[j - 1])\n",
    "\n",
    "        return new_target_distr\n",
    "        \n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state, show=False):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > 10 * self.batch_size:\n",
    "\n",
    "            if self.counter % self.period == 0:\n",
    "                self.freezing_q_function.load_state_dict(self.q_function.state_dict())\n",
    "            \n",
    "            self.counter += 1\n",
    "\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, list(zip(*batch)))\n",
    "            states, actions, rewards, dones, next_states = states.to(device), actions.unsqueeze(1).to(device), rewards.unsqueeze(dim=1), dones.unsqueeze(dim=1) , next_states.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                one_hot_actions = np.zeros((self.batch_size, self.action_dim))\n",
    "                q_values = None\n",
    "                for i in range(self.action_dim):\n",
    "                    one_hot_actions[:, i] = 1\n",
    "                    next_actions = torch.FloatTensor(one_hot_actions).to(device)\n",
    "                    reward_distrs = self.freezing_q_function(next_states, next_actions)\n",
    "                    reward_distrs = reward_distrs\n",
    "                    q_value = torch.sum(torch.tensor(self.atoms) * reward_distrs, dim=1, keepdim=True)\n",
    "                    q_values = q_value if q_values is None else torch.cat((q_values, q_value), dim=1)\n",
    "                    one_hot_actions[:, i] = 0\n",
    "\n",
    "                    \n",
    "                argmax_actions = torch.argmax(q_values, dim=1)\n",
    "                if show:\n",
    "                    print(f\"q_values: {q_values}\")\n",
    "                    print(f\"argmax_actions: {argmax_actions}\")\n",
    "                \n",
    "                one_hot_actions[np.arange(self.batch_size), argmax_actions] = 1\n",
    "                max_one_hot_actions = torch.FloatTensor(one_hot_actions)\n",
    "                reward_distrs = self.freezing_q_function(next_states.float(), max_one_hot_actions).cpu().numpy()\n",
    "                atoms = torch.tensor(self.atoms).repeat(self.batch_size, 1)\n",
    "                targets = rewards + self.gamma * (1 - dones) * atoms\n",
    "                # print(f\"reward_distrs: {reward_distrs}\")\n",
    "                # print(f\"targets: {targets}\")\n",
    "                # print(reward_distrs.shape)\n",
    "                projected_targets = [self.project(targets[k], reward_distrs[k]) for k in range(self.batch_size)]\n",
    "                projected_targets = torch.FloatTensor(projected_targets).to(device)\n",
    "                if show:\n",
    "                    print(f\"atoms: {atoms}\")\n",
    "                    print(f\"targets: {targets}\")\n",
    "                    print(f\"projected_targets: {projected_targets}\")\n",
    "                # print(f\"projected: {projected_targets}\")\n",
    "                \n",
    "\n",
    "            one_hot_actions = np.zeros((self.batch_size, self.action_dim))\n",
    "            # была ошибка здесь. Отдавал вместо actions - argmax_actions.\n",
    "            one_hot_actions[np.arange(self.batch_size), actions.long()] = 1\n",
    "            one_hot_actions = torch.FloatTensor(one_hot_actions)\n",
    "            distrs = self.q_function(states.float(), one_hot_actions)\n",
    "            loss = - torch.mean(torch.sum(projected_targets.detach() * torch.log(distrs + 1e-8), dim=1, keepdim=True))\n",
    "\n",
    "            self.optimzaer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimzaer.step()\n",
    "\n",
    "            if show:\n",
    "                print(f\"cross-entorpys: {projected_targets.detach() * torch.log(distrs + 1e-8)}\")\n",
    "                print(f\"loss: {loss}\")\n",
    "\n",
    "    def decrease_params(self):\n",
    "        self.epsilon = max(self.epsilon - self.epsilon_decrease, self.epsilon_min)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "88162521-0874-4605-9538-0e6f1c8eabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_learning(env, agent, episode_n = 100, t_max = 500):\n",
    "    # agent.epsilon_decrease = 1.0 / (0.75*episode_n)\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "        total_reward = 0\n",
    "\n",
    "        show = False\n",
    "        if episode % 50 == 0:\n",
    "            show = False\n",
    "                \n",
    "        state, info = env.reset()\n",
    "        for t in range(t_max):\n",
    "                \n",
    "            action = agent.get_action(state, show)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "\n",
    "            if show:\n",
    "                print(f\"iteration: {t}\")\n",
    "            \n",
    "            agent.fit(state, action, reward, terminated or truncated, next_state, show)\n",
    "            \n",
    "    \n",
    "            state = next_state\n",
    "    \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(total_reward)\n",
    "        if episode % 10 == 0:\n",
    "            print(f'episode: {episode}, total_reward: {np.mean(total_rewards[-10:])}')\n",
    "            \n",
    "        agent.decrease_params()\n",
    "    \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae2b6b26-e7be-4ae7-8199-c4ecbfa058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проблема при реализации была в том, что в модели добавились новые слои, а я эти слои не копировал, а копировал только общий позвоночник модели как в старой версии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d226d147-020c-4efa-82d5-01033ef86db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.81239525796218e-16"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33d58f5d-03e4-404e-b8aa-47efaa3e4211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "episode: 0, total_reward: 10.0\n",
      "episode: 10, total_reward: 24.4\n",
      "episode: 20, total_reward: 21.4\n",
      "episode: 30, total_reward: 26.2\n",
      "[0.465 0.535]\n",
      "episode: 40, total_reward: 25.3\n",
      "[0.45 0.55]\n",
      "episode: 50, total_reward: 35.2\n",
      "episode: 60, total_reward: 24.0\n",
      "[0.568 0.432]\n",
      "episode: 70, total_reward: 41.4\n",
      "episode: 80, total_reward: 29.8\n",
      "[0.418 0.582]\n",
      "episode: 90, total_reward: 35.2\n",
      "[0.405 0.595]\n",
      "episode: 100, total_reward: 32.1\n",
      "episode: 110, total_reward: 33.4\n",
      "[0.611 0.389]\n",
      "episode: 120, total_reward: 50.8\n",
      "[0.621 0.379]\n",
      "episode: 130, total_reward: 42.3\n",
      "[0.632 0.368]\n",
      "episode: 140, total_reward: 54.3\n",
      "[0.642 0.358]\n",
      "[0.35 0.65]\n",
      "episode: 150, total_reward: 56.9\n",
      "[0.34 0.66]\n",
      "episode: 160, total_reward: 50.5\n",
      "episode: 170, total_reward: 28.6\n",
      "[0.675 0.325]\n",
      "episode: 180, total_reward: 37.4\n",
      "episode: 190, total_reward: 24.0\n",
      "[0.693 0.307]\n",
      "episode: 200, total_reward: 35.8\n",
      "[0.292 0.708]\n",
      "episode: 210, total_reward: 32.4\n",
      "[0.719 0.281]\n",
      "episode: 220, total_reward: 42.8\n",
      "[0.729 0.271]\n",
      "episode: 230, total_reward: 62.8\n",
      "[0.735 0.265]\n",
      "episode: 240, total_reward: 56.0\n",
      "[0.746 0.254]\n",
      "episode: 250, total_reward: 60.5\n",
      "[0.245 0.755]\n",
      "episode: 260, total_reward: 55.7\n",
      "[0.237 0.763]\n",
      "episode: 270, total_reward: 51.7\n",
      "[0.228 0.772]\n",
      "[0.223 0.777]\n",
      "episode: 280, total_reward: 79.0\n",
      "[0.214 0.786]\n",
      "episode: 290, total_reward: 74.5\n",
      "[0.209 0.791]\n",
      "[0.797 0.203]\n",
      "episode: 300, total_reward: 70.7\n",
      "[0.804 0.196]\n",
      "[0.81 0.19]\n",
      "episode: 310, total_reward: 88.9\n",
      "[0.817 0.183]\n",
      "episode: 320, total_reward: 79.0\n",
      "[0.177 0.823]\n",
      "[0.83 0.17]\n",
      "episode: 330, total_reward: 68.2\n",
      "[0.164 0.836]\n",
      "episode: 340, total_reward: 82.9\n",
      "[0.842 0.158]\n",
      "[0.847 0.153]\n",
      "episode: 350, total_reward: 96.1\n",
      "[0.854 0.146]\n",
      "[0.14 0.86]\n",
      "episode: 360, total_reward: 76.6\n",
      "[0.135 0.865]\n",
      "[0.87 0.13]\n",
      "episode: 370, total_reward: 99.9\n",
      "[0.124 0.876]\n",
      "episode: 380, total_reward: 85.3\n",
      "[0.119 0.881]\n",
      "[0.886 0.114]\n",
      "episode: 390, total_reward: 96.1\n",
      "[0.108 0.892]\n",
      "[0.101 0.899]\n",
      "episode: 400, total_reward: 73.5\n",
      "[0.905 0.095]\n",
      "[0.91 0.09]\n",
      "episode: 410, total_reward: 90.8\n",
      "[0.916 0.084]\n",
      "[0.92 0.08]\n",
      "episode: 420, total_reward: 100.9\n",
      "[0.075 0.925]\n",
      "[0.926 0.074]\n",
      "[0.93 0.07]\n",
      "episode: 430, total_reward: 152.4\n",
      "[0.935 0.065]\n",
      "[0.937 0.063]\n",
      "[0.939 0.061]\n",
      "[0.06 0.94]\n",
      "episode: 440, total_reward: 202.4\n",
      "[0.943 0.057]\n",
      "[0.947 0.053]\n",
      "episode: 450, total_reward: 140.5\n",
      "[0.049 0.951]\n",
      "[0.953 0.047]\n",
      "[0.954 0.046]\n",
      "[0.957 0.043]\n",
      "episode: 460, total_reward: 189.9\n",
      "[0.962 0.038]\n",
      "[0.037 0.963]\n",
      "[0.965 0.035]\n",
      "[0.034 0.966]\n",
      "[0.033 0.967]\n",
      "[0.968 0.032]\n",
      "[0.03 0.97]\n",
      "episode: 470, total_reward: 324.1\n",
      "[0.972 0.028]\n",
      "[0.973 0.027]\n",
      "[0.026 0.974]\n",
      "[0.025 0.975]\n",
      "[0.024 0.976]\n",
      "[0.022 0.978]\n",
      "[0.979 0.021]\n",
      "episode: 480, total_reward: 361.6\n",
      "[0.981 0.019]\n",
      "[0.984 0.016]\n",
      "[0.015 0.985]\n",
      "[0.013 0.987]\n",
      "[0.989 0.011]\n",
      "episode: 490, total_reward: 253.2\n",
      "[0.992 0.008]\n",
      "[0.006 0.994]\n",
      "[0.996 0.004]\n",
      "[0.001 0.999]\n",
      "episode: 500, total_reward: 200.3\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 510, total_reward: 252.6\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 520, total_reward: 259.1\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 530, total_reward: 216.4\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 540, total_reward: 203.0\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 550, total_reward: 147.1\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 560, total_reward: 148.0\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 570, total_reward: 149.5\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 580, total_reward: 159.6\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 590, total_reward: 130.6\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 600, total_reward: 176.8\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 610, total_reward: 178.8\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 620, total_reward: 161.4\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 630, total_reward: 119.2\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 640, total_reward: 145.7\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 650, total_reward: 126.6\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 660, total_reward: 131.9\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 670, total_reward: 116.1\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 680, total_reward: 167.4\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 690, total_reward: 258.0\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 700, total_reward: 238.6\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 710, total_reward: 184.2\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 720, total_reward: 130.9\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 730, total_reward: 201.7\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 740, total_reward: 126.8\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 750, total_reward: 102.7\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 760, total_reward: 78.0\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 770, total_reward: 87.5\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 780, total_reward: 89.8\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 790, total_reward: 110.2\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 800, total_reward: 177.4\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 810, total_reward: 156.7\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 820, total_reward: 122.1\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 830, total_reward: 153.9\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 840, total_reward: 266.2\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 850, total_reward: 236.5\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 860, total_reward: 235.6\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 870, total_reward: 286.9\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 880, total_reward: 216.8\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 890, total_reward: 135.0\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 900, total_reward: 120.2\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 910, total_reward: 123.8\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 920, total_reward: 123.5\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 930, total_reward: 136.6\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[9.9995e-01 5.0000e-05]\n",
      "episode: 940, total_reward: 149.7\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "episode: 950, total_reward: 131.2\n",
      "[9.9995e-01 5.0000e-05]\n",
      "[5.0000e-05 9.9995e-01]\n",
      "[9.9995e-01 5.0000e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m episode_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m\n\u001b[1;32m     19\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN_double(state_dim, action_dim, n_atoms\u001b[38;5;241m=\u001b[39mn_atoms, v_min\u001b[38;5;241m=\u001b[39mv_min, v_max\u001b[38;5;241m=\u001b[39mv_max)\n\u001b[0;32m---> 21\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mDQN_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_n\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 24\u001b[0m, in \u001b[0;36mDQN_learning\u001b[0;34m(env, agent, episode_n, t_max)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtruncated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[79], line 160\u001b[0m, in \u001b[0;36mDQN_double.fit\u001b[0;34m(self, state, action, reward, done, next_state, show)\u001b[0m\n\u001b[1;32m    156\u001b[0m targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m atoms\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# print(f\"reward_distrs: {reward_distrs}\")\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(f\"targets: {targets}\")\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# print(reward_distrs.shape)\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m projected_targets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(targets[k], reward_distrs[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[1;32m    161\u001b[0m projected_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(projected_targets)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show:\n",
      "Cell \u001b[0;32mIn[79], line 160\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    156\u001b[0m targets \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m atoms\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# print(f\"reward_distrs: {reward_distrs}\")\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(f\"targets: {targets}\")\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# print(reward_distrs.shape)\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m projected_targets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_distrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[1;32m    161\u001b[0m projected_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(projected_targets)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show:\n",
      "Cell \u001b[0;32mIn[79], line 114\u001b[0m, in \u001b[0;36mDQN_double.project\u001b[0;34m(self, values, distr)\u001b[0m\n\u001b[1;32m    112\u001b[0m             j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    113\u001b[0m         new_target_distr[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m distr[i] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms[j] \u001b[38;5;241m-\u001b[39m values[i]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms[j] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]) \n\u001b[0;32m--> 114\u001b[0m         new_target_distr[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m distr[i] \u001b[38;5;241m*\u001b[39m (values[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms[j] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms[j \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_target_distr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers import TransformReward\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "v_min = 1\n",
    "v_max = 100\n",
    "n_atoms = 51\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = TransformReward(env, lambda r: v_min + r * (v_max - v_min) / 500.0)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "\n",
    "episode_n=3000\n",
    "\n",
    "\n",
    "agent = DQN_double(state_dim, action_dim, n_atoms=n_atoms, v_min=v_min, v_max=v_max)\n",
    "\n",
    "total_rewards = DQN_learning(env, agent, episode_n=episode_n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
