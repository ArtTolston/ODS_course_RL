{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e927c7b1-6575-4a8c-b5e8-01d88249ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import play \n",
    "from gym import wrappers\n",
    "from gym.wrappers import GrayScaleObservation, RecordEpisodeStatistics, TimeLimit, ResizeObservation, FrameStack\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from hashlib import md5\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tensordict import TensorDict\n",
    "\n",
    "import torchrl\n",
    "from torchrl.data.replay_buffers import PrioritizedReplayBuffer, ReplayBuffer, PrioritizedSampler\n",
    "from torchrl.data import TensorDictPrioritizedReplayBuffer, LazyMemmapStorage, TensorDictReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2898d4-2f11-4042-9733-87f4f9268bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6b5036-b478-4204-ad78-42cec80bc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe5c9fd-848a-42d6-b33e-c29e8d96e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskVelocityWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Gym environment observation wrapper used to mask velocity terms in\n",
    "    observations. The intention is the make the MDP partially observatiable.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(MaskVelocityWrapper, self).__init__(env)\n",
    "        if ENV == \"CartPole-v1\":\n",
    "            self.mask = np.array([1., 0., 1., 0.])\n",
    "        elif ENV == \"Pendulum-v0\":\n",
    "            self.mask = np.array([1., 1., 0.])\n",
    "        elif ENV == \"LunarLander-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        elif ENV == \"LunarLanderContinuous-v2\":\n",
    "            self.mask = np.array([1., 1., 0., 0., 1., 0., 1., 1,])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return  observation * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30a9b15d-731b-4f8f-bde5-97b01113e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Успешный вариант. Дошел до отметки 400.\n",
    "# class DRQNAgent(nn.Module):\n",
    "#     def __init__(self, input_shape, action_n, lr=1e-3, gamma=0.95, batch_size=5, period=10, N=20, M=0, episode_n=1000):\n",
    "#         super().__init__()\n",
    "# class RQfunction(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, hidden_size=16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30561323-8533-41c0-a7f1-a0f08707870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=32, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(state_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.LayerNorm(hidden_size), # перед RELU или ELU нет смысла ставить Norm так как градиенты пройдут нормально и без этого через эти функции активации\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.prev_hidden_state = None\n",
    "        self.hidden_state = None\n",
    "        self.current_batch_size = 0\n",
    "\n",
    "    def initialize(self, batch_size):\n",
    "        self.current_batch_size = batch_size\n",
    "        if batch_size == 0:\n",
    "            self.hidden_state = (torch.zeros(self.num_layers, self.hidden_size).to(device), torch.zeros(self.num_layers, self.hidden_size).to(device))\n",
    "        # else:\n",
    "        #     self.hidden_state = (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device), torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "        # self.prev_hidden_state = None\n",
    "\n",
    "    def forward(self, x, batch_size=None):\n",
    "        if len(x.shape) == 3:\n",
    "            batch_size = x.shape[0]\n",
    "        if len(x.shape) == 2:\n",
    "            batch_size = 0\n",
    "        if self.hidden_state is None or batch_size != self.current_batch_size:\n",
    "            self.initialize(batch_size)\n",
    "\n",
    "        self.prev_hidden_state = self.hidden_state\n",
    "        # with torch.no_grad():\n",
    "        output, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        # if self.current_batch_size == 0:\n",
    "        #     print(f\"obs: {x}\")\n",
    "        #     print(f\"state: {output}\")\n",
    "        q_values = self.q(output)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# class RecurrentActor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, hidden_size=64, batch_size=64):\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64d81145-3912-42d0-ab8f-1305f1e068b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQNAgent(nn.Module):\n",
    "    def __init__(self, input_shape, action_n, lr=1e-3, gamma=0.97, batch_size=8, period=10, N=20, M=0, episode_n=1000):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.q_function = RQfunction(input_shape, action_n).to(device)\n",
    "        self.target_q_function = RQfunction(input_shape, action_n).to(device)\n",
    "        self.update_weights()\n",
    "\n",
    "        self.epsilon_min = 0.02\n",
    "        self.epsilon_decay = 0.5 * 1.0 / (episode_n)\n",
    "        self.epsilon = 0.9\n",
    "\n",
    "        self.episode_n = episode_n\n",
    "        self.current_episode = 0\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.period = period\n",
    "        self.counter = 1\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[int(0.7 * self.episode_n)], gamma=0.5)\n",
    "\n",
    "        # self.capacity = 100_000\n",
    "        # storage = LazyMemmapStorage(self.capacity, scratch_dir='/home/artem/atari_games/tmp/')\n",
    "        # self.sampler = PrioritizedSampler(self.capacity, alpha=self.alpha, beta=self.beta)\n",
    "        # self.tdrb = TensorDictReplayBuffer(storage=storage, sampler=self.sampler, priority_key='td_error')\n",
    "        self.states_count = 0\n",
    "        self.rb = []\n",
    "\n",
    "        self.states = None\n",
    "        self.hidden_states =  None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.dones = None\n",
    "        self.next_states = None\n",
    "        self.next_hidden_states = None\n",
    "\n",
    "    def save_model(self, path=f'/home/artem/atari_games/models/DRQN_{md5(str(time.time()).encode()).hexdigest()}.pth'):\n",
    "        state = {\n",
    "            'model_dict': self.q_function.state_dict(),\n",
    "            'optimizer_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "\n",
    "    def load_model(self,path=f'/home/artem/atari_games/models/DRQN_{md5(str(time.time()).encode()).hexdigest()}'):\n",
    "        if os.path.exists(path):\n",
    "            state = torch.load(path)\n",
    "            self.q_function.rnn.load_state_dict(state['model_dict'])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max((self.epsilon - self.epsilon_decay), self.epsilon_min)\n",
    "\n",
    "    def decay_all(self):\n",
    "        self.decay_epsilon()\n",
    "\n",
    "    def e_greedy_action(self, q_values):\n",
    "        probs = np.ones(self.action_n) * self.epsilon / self.action_n\n",
    "        probs[np.argmax(q_values.cpu().numpy())] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_n), p=probs)\n",
    "        return action\n",
    "        \n",
    "    def get_action(self, obs: np.ndarray) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float).unsqueeze(dim=0).to(device)\n",
    "            q_values = self.q_function(obs).squeeze()\n",
    "            if self.counter % 2500 == 0:\n",
    "                print(q_values)\n",
    "            action = self.e_greedy_action(q_values)\n",
    "            return action\n",
    "\n",
    "    def add_sample(self, state, action, reward, next_state, done):\n",
    "        # print(f\"stacked {torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=0)}\")\n",
    "        # print(f\"hidden_states {0 if self.hidden_states is None else self.hidden_states.shape}\")\n",
    "\n",
    "        stacked_hidden_state = torch.stack((self.q_function.prev_hidden_state[0], self.q_function.prev_hidden_state[1]), dim=0)\n",
    "        stacked_next_hidden_state = torch.stack((self.q_function.hidden_state[0], self.q_function.hidden_state[1]), dim=0)\n",
    "        \n",
    "\n",
    "        self.states = torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0) if self.states is None else torch.cat((self.states, torch.tensor(state).unsqueeze(dim=0)), dim=0)\n",
    "        self.hidden_states = stacked_hidden_state.unsqueeze(dim=0) if self.hidden_states is None else torch.cat((self.hidden_states, stacked_hidden_state.unsqueeze(dim=0)), dim=0)\n",
    "        self.actions = torch.tensor(action).unsqueeze(dim=0) if self.actions is None else torch.cat((self.actions, torch.tensor(action).unsqueeze(dim=0)), dim=0)\n",
    "        self.rewards = torch.tensor(reward).unsqueeze(dim=0) if self.rewards is None else torch.cat((self.rewards, torch.tensor(reward).unsqueeze(dim=0)), dim=0)\n",
    "        self.dones = torch.tensor(int(done)).unsqueeze(dim=0) if self.dones is None else torch.cat((self.dones, torch.tensor(int(done)).unsqueeze(dim=0)), dim=0)\n",
    "        self.next_states = torch.tensor(next_state, dtype=torch.float32).unsqueeze(dim=0) if self.next_states is None else torch.cat((self.next_states, torch.tensor(next_state).unsqueeze(dim=0)), dim=0)\n",
    "        self.next_hidden_states = stacked_next_hidden_state.unsqueeze(dim=0) if self.next_hidden_states is None else torch.cat((self.next_hidden_states, stacked_next_hidden_state.unsqueeze(dim=0)), dim=0)\n",
    "        \n",
    "        self.states_count += 1\n",
    "\n",
    "        if done or self.states_count % self.N == 0:\n",
    "            # print(self.states.shape)\n",
    "            self.rb.append(\n",
    "                {\n",
    "                    'state': self.states.to(device),#.clone().detach().requires_grad_(False) ,\n",
    "                    'hidden_state': self.hidden_states.to(device),\n",
    "                    'action': self.actions.to(device),\n",
    "                    'reward': self.rewards.to(device),\n",
    "                    'next_state': self.next_states.to(device),\n",
    "                    'next_hidden_state': self.next_hidden_states.to(device),\n",
    "                    'done': self.dones.to(device)\n",
    "                        }\n",
    "            )\n",
    "\n",
    "            self.states = None\n",
    "            self.hidden_states =  None\n",
    "            self.actions = None\n",
    "            self.rewards = None\n",
    "            self.dones = None\n",
    "            self.next_states = None\n",
    "            self.next_hidden_states = None\n",
    "\n",
    "        if done:\n",
    "            self.current_episode += 1\n",
    "            self.decay_all()\n",
    "            self.q_function.prev_hidden_state = None\n",
    "            self.q_function.hidden_state = None\n",
    "\n",
    "    def update_weights(self):\n",
    "        for parameter_freeze, parameter in zip(self.target_q_function.rnn.parameters(), self.q_function.rnn.parameters()):\n",
    "            with torch.no_grad():\n",
    "                parameter_freeze.data.copy_(parameter.data)\n",
    "        for parameter_freeze, parameter in zip(self.target_q_function.q.parameters(), self.q_function.q.parameters()):\n",
    "            with torch.no_grad():\n",
    "                parameter_freeze.data.copy_(parameter.data)\n",
    "\n",
    "    def fit(self):\n",
    "        if self.batch_size < len(self.rb):\n",
    "            if self.counter % self.period == 0:\n",
    "                # print('weights change')\n",
    "                # не уверен, что LSTM можно так легко скопировать\n",
    "                self.update_weights()\n",
    "                \n",
    "            self.counter += 1\n",
    "\n",
    "            sample = random.sample(self.rb, self.batch_size)\n",
    "\n",
    "            flag = True\n",
    "            for rollout in sample:\n",
    "                # print(rollout['state'])\n",
    "                # вычисляем таргеты\n",
    "                self.q_function.hidden_state = (rollout['hidden_state'][0][0], rollout['hidden_state'][0][1])\n",
    "                self.target_q_function.hidden_state = (rollout['hidden_state'][0][0], rollout['hidden_state'][0][1])\n",
    "                # попробую инициализировать нулями hidden_state не очень пошло\n",
    "                # self.q_function.initialize(1)\n",
    "                # self.target_q_function.initialize(1)\n",
    "\n",
    "                # print(f\"q_func: {self.target_q_function(rollout['next_state'])}\")\n",
    "                targets = rollout['reward'].unsqueeze(dim=1) + (1 - rollout['done'].unsqueeze(dim=1)) * self.gamma * self.target_q_function(rollout['next_state'].float())\\\n",
    "                .gather(1, torch.argmax(self.q_function(rollout['next_state'].float()), dim=1).unsqueeze(dim=1))\n",
    "\n",
    "                \n",
    "                # print(f\"done: {(1 - rollout['done']).shape}\")\n",
    "                # print(f\"shape check: {torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1).shape}\")\n",
    "                # print(f\"target q: {self.target_q_function(rollout['next_state']).shape}\")\n",
    "                # print(f\"the whole: {self.target_q_function(rollout['next_state']).gather(1, torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1)).shape}\")\n",
    "                # print(f\"targets check: {targets.shape}\")\n",
    "                # вычисляем loss \n",
    "                q_values = self.q_function(rollout['state'].float()).gather(1, rollout['action'].unsqueeze(dim=1))\n",
    "                print(rollout['state'].float().shape)\n",
    "\n",
    "                # получаем последние  N - M состояний, так как их отображение q_values более правдоподобно из-за прогрева hidden_state\n",
    "                \n",
    "                td = (q_values - targets.detach()) ** 2\n",
    "                if td.shape[0] > self.M and self.M != 0:\n",
    "                    zero_mask = torch.zeros((self.M, 1))\n",
    "                    ones_mask = torch.ones((td.shape[0] - self.M, 1))\n",
    "                    mask = torch.cat((zero_mask, ones_mask), dim=0).to(device)\n",
    "                    td *= mask\n",
    "                loss = torch.mean(td)\n",
    "                loss.backward()\n",
    "\n",
    "                # nn.utils.clip_grad_norm_(self.q_function.q.parameters(), max_norm=100.0)\n",
    "                # nn.utils.clip_grad_norm_(self.q_function.rnn.parameters(), max_norm=100.0)\n",
    "\n",
    "                if self.counter % 2500 == 0 and flag:\n",
    "                    print(f\"targets: {targets}\")\n",
    "                    print(f\"q_values: {q_values}\")\n",
    "                    print(f\"loss: {loss}\")\n",
    "                    flag = False\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            self.scheduler.step()\n",
    "            self.q_function.initialize(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9df29599-aeec-4cd1-a77c-92ea30f526b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DRQNAgent(nn.Module):\n",
    "#     def __init__(self, input_shape, action_n, lr=1e-3, gamma=0.95, batch_size=2, period=500, N=50, M=10, episode_n=1000):\n",
    "# рабочий вариант на 3к операциях почти сошелся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "803f6120-f0d2-43da-839c-a0158575c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# делаем пошаговые эксперименты.\n",
    "# уменьшил период до 5 \n",
    "# маленький период не способствует улучшению\n",
    "\n",
    "# увеличил длину траектории с 20 до 30\n",
    "# показал примерно такой же результат\n",
    "\n",
    "# еще хочется потом уменьшить hidden size до 8 так как и такого размера по моему мнению должно быть достаточно\n",
    "# не сработало\n",
    "\n",
    "# оставил только те роллауты что по длине достаточно длинные\n",
    "\n",
    "# сейчас попробую сделать мултилэйер LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69d025-4f37-41ae-bed1-ab173e5d75e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "787fcff5-a260-49fa-8607-37747e88f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0  mean last ten: 12.0\n",
      "episode: 10  mean last ten: 24.8\n",
      "episode: 20  mean last ten: 29.2\n",
      "episode: 30  mean last ten: 19.2\n",
      "episode: 40  mean last ten: 24.2\n",
      "episode: 50  mean last ten: 14.6\n",
      "episode: 60  mean last ten: 23.1\n",
      "episode: 70  mean last ten: 25.3\n",
      "episode: 80  mean last ten: 22.3\n",
      "episode: 90  mean last ten: 28.0\n",
      "episode: 100  mean last ten: 27.1\n",
      "episode: 110  mean last ten: 23.9\n",
      "episode: 120  mean last ten: 23.8\n",
      "episode: 130  mean last ten: 22.8\n",
      "episode: 140  mean last ten: 20.9\n",
      "episode: 150  mean last ten: 20.9\n",
      "episode: 160  mean last ten: 19.5\n",
      "episode: 170  mean last ten: 19.1\n",
      "episode: 180  mean last ten: 18.5\n",
      "episode: 190  mean last ten: 22.7\n",
      "episode: 200  mean last ten: 18.0\n",
      "episode: 210  mean last ten: 19.0\n",
      "episode: 220  mean last ten: 23.1\n",
      "episode: 230  mean last ten: 20.0\n",
      "targets: tensor([[4.3312],\n",
      "        [1.8377],\n",
      "        [2.8917],\n",
      "        [1.0000]], grad_fn=<AddBackward0>)\n",
      "q_values: tensor([[4.2395],\n",
      "        [1.6095],\n",
      "        [1.7319],\n",
      "        [0.5307]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.40647953748703003\n",
      "tensor([3.8296, 3.7504])\n",
      "tensor([3.7641, 3.7379])\n",
      "episode: 240  mean last ten: 16.8\n",
      "episode: 250  mean last ten: 15.8\n",
      "episode: 260  mean last ten: 23.0\n",
      "episode: 270  mean last ten: 17.4\n",
      "episode: 280  mean last ten: 22.6\n",
      "episode: 290  mean last ten: 23.4\n",
      "episode: 300  mean last ten: 19.6\n",
      "episode: 310  mean last ten: 21.6\n",
      "episode: 320  mean last ten: 21.8\n",
      "episode: 330  mean last ten: 22.1\n",
      "episode: 340  mean last ten: 19.9\n",
      "episode: 350  mean last ten: 19.4\n",
      "episode: 360  mean last ten: 22.4\n",
      "episode: 370  mean last ten: 16.9\n",
      "episode: 380  mean last ten: 21.1\n",
      "episode: 390  mean last ten: 16.2\n",
      "episode: 400  mean last ten: 25.1\n",
      "episode: 410  mean last ten: 17.4\n",
      "episode: 420  mean last ten: 21.2\n",
      "episode: 430  mean last ten: 17.6\n",
      "episode: 440  mean last ten: 22.9\n",
      "episode: 450  mean last ten: 17.0\n",
      "episode: 460  mean last ten: 20.4\n",
      "episode: 470  mean last ten: 16.9\n",
      "targets: tensor([[5.7042],\n",
      "        [6.2545],\n",
      "        [6.2461],\n",
      "        [6.2184],\n",
      "        [5.5374],\n",
      "        [4.3356],\n",
      "        [2.8103],\n",
      "        [1.7422],\n",
      "        [1.0000]], grad_fn=<AddBackward0>)\n",
      "q_values: tensor([[5.5389],\n",
      "        [5.7709],\n",
      "        [5.6339],\n",
      "        [5.2917],\n",
      "        [5.0559],\n",
      "        [3.9920],\n",
      "        [2.9712],\n",
      "        [1.9128],\n",
      "        [1.0279]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.2111733853816986\n",
      "tensor([4.8639, 4.8886])\n",
      "tensor([6.0775, 6.0908])\n",
      "episode: 480  mean last ten: 25.2\n",
      "episode: 490  mean last ten: 19.6\n",
      "episode: 500  mean last ten: 19.7\n",
      "episode: 510  mean last ten: 19.8\n",
      "episode: 520  mean last ten: 18.9\n",
      "episode: 530  mean last ten: 16.0\n",
      "episode: 540  mean last ten: 20.1\n",
      "episode: 550  mean last ten: 22.3\n",
      "episode: 560  mean last ten: 29.3\n",
      "episode: 570  mean last ten: 21.1\n",
      "episode: 580  mean last ten: 24.7\n",
      "episode: 590  mean last ten: 31.8\n",
      "episode: 600  mean last ten: 22.4\n",
      "episode: 610  mean last ten: 17.3\n",
      "episode: 620  mean last ten: 26.3\n",
      "episode: 630  mean last ten: 17.4\n",
      "episode: 640  mean last ten: 22.2\n",
      "episode: 650  mean last ten: 22.7\n",
      "episode: 660  mean last ten: 22.4\n",
      "episode: 670  mean last ten: 17.8\n",
      "episode: 680  mean last ten: 20.7\n",
      "episode: 690  mean last ten: 22.5\n",
      "episode: 700  mean last ten: 19.0\n",
      "episode: 710  mean last ten: 21.8\n",
      "targets: tensor([[4.3990],\n",
      "        [5.6018],\n",
      "        [5.7840],\n",
      "        [6.2158],\n",
      "        [6.5170],\n",
      "        [6.6386],\n",
      "        [6.7119],\n",
      "        [6.7988],\n",
      "        [6.4716],\n",
      "        [6.4378],\n",
      "        [6.5867],\n",
      "        [6.8133]], grad_fn=<AddBackward0>)\n",
      "q_values: tensor([[4.2309],\n",
      "        [5.7468],\n",
      "        [5.7287],\n",
      "        [6.0152],\n",
      "        [6.1643],\n",
      "        [6.3594],\n",
      "        [6.5034],\n",
      "        [6.6322],\n",
      "        [6.7594],\n",
      "        [6.6877],\n",
      "        [6.7812],\n",
      "        [7.0020]], grad_fn=<GatherBackward0>)\n",
      "loss: 0.048741620033979416\n",
      "tensor([4.3588, 4.4823])\n",
      "tensor([5.2519, 5.2744])\n",
      "episode: 720  mean last ten: 23.4\n",
      "episode: 730  mean last ten: 20.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     42\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[30], line 154\u001b[0m, in \u001b[0;36mDRQNAgent.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_q_function\u001b[38;5;241m.\u001b[39mhidden_state \u001b[38;5;241m=\u001b[39m (rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# попробую инициализировать нулями hidden_state не очень пошло\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# self.q_function.initialize(1)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# self.target_q_function.initialize(1)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# print(f\"q_func: {self.target_q_function(rollout['next_state'])}\")\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m targets \u001b[38;5;241m=\u001b[39m rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_q_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnext_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m    155\u001b[0m \u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, torch\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_function(rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_state\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(f\"done: {(1 - rollout['done']).shape}\")\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# print(f\"shape check: {torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1).shape}\")\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# print(f\"target q: {self.target_q_function(rollout['next_state']).shape}\")\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# print(f\"the whole: {self.target_q_function(rollout['next_state']).gather(1, torch.argmax(self.q_function(rollout['next_state']), dim=1).unsqueeze(dim=1)).shape}\")\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# print(f\"targets check: {targets.shape}\")\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# вычисляем loss \u001b[39;00m\n\u001b[1;32m    164\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_function(rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, rollout[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 39\u001b[0m, in \u001b[0;36mRQfunction.forward\u001b[0;34m(self, x, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# if self.current_batch_size == 0:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     print(f\"obs: {x}\")\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     print(f\"state: {output}\")\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Environment parameters\n",
    "ENV = \"CartPole-v1\"\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# ENV = \"LunarLander-v2\"\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# убираем скорость из состояний\n",
    "env = MaskVelocityWrapper(env)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "episode_n = 10_000\n",
    "\n",
    "agent = DRQNAgent(state_dim, action_dim, episode_n=episode_n)\n",
    "# agent.load_model(\"pretrained_lstm.ptx\")\n",
    "\n",
    "total_rewards = []\n",
    "loss1 = []\n",
    "grads1 = []\n",
    "loss2 = []\n",
    "grads2 = []\n",
    "counter = 0\n",
    "for episode in range(episode_n):\n",
    "\n",
    "    total_reward = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        agent.add_sample(state, action, reward, next_state, terminated or truncated)\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 2 == 0:\n",
    "            agent.fit()\n",
    "    \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    \n",
    "        \n",
    "    total_rewards.append(total_reward)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"episode: {episode}  mean last ten: {np.mean(total_rewards[-10:])}\")\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ea6c970-b5b9-43ca-90d9-c26496ba5a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 4]).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9d122ca-0f32-45a9-9a75-2fe49e131fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 1)\n",
    "input = torch.randn(1, 10)\n",
    "h0 = torch.randn(1, 20)\n",
    "c0 = torch.randn(1, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70787dd0-9de1-4bd6-b0ad-a8f567fdc42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1831,  0.2089,  0.4142, -0.0557, -0.0669,  0.3513, -0.1570, -0.2058,\n",
       "          0.1245,  0.1779,  0.2874, -0.1077, -0.2030, -0.0157, -0.0557,  0.1683,\n",
       "          0.0930,  0.0799, -0.3804,  0.1875]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "09077e12-3d59-4c1e-8256-ea329000b813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1831,  0.2089,  0.4142, -0.0557, -0.0669,  0.3513, -0.1570, -0.2058,\n",
       "          0.1245,  0.1779,  0.2874, -0.1077, -0.2030, -0.0157, -0.0557,  0.1683,\n",
       "          0.0930,  0.0799, -0.3804,  0.1875]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ff6f1269-dab1-4206-ab7b-d4f10a3d9066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3891,  0.4474,  1.2947, -0.1222, -0.1394,  1.9529, -0.2809, -0.3401,\n",
       "          0.3209,  0.8142,  0.6584, -0.2212, -0.4863, -0.0372, -0.0857,  0.6757,\n",
       "          0.2002,  0.1816, -0.9577,  0.4648]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "865664d4-4ce8-40aa-981e-7b155f531a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
