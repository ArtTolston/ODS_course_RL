{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890dd138-a5db-4a95-926c-16bce66fedcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/base/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e565e0c2-d0c2-48ef-8f8d-f1380aa5b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a171eb6a-8cf6-4b56-989c-743b8f38a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_levels):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, n_levels * action_dim)\n",
    "        )\n",
    "        self.action_dim = action_dim\n",
    "        self.n_levels = n_levels\n",
    "\n",
    "    def forward(self, states):\n",
    "        batch_size = states.shape[0]\n",
    "        rewards = self.model(states)\n",
    "        actions_rewards = rewards.reshape((batch_size, self.action_dim, self.n_levels))\n",
    "        return actions_rewards\n",
    "\n",
    "\n",
    "class DQN_double:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 gamma=0.95,\n",
    "                 lr=1e-3,\n",
    "                 batch_size=2,\n",
    "                 epsilon_decrease=0.005,\n",
    "                 epsilon_min=0.0001,\n",
    "                 period=100,\n",
    "                 n_levels = 5\n",
    "                ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = Qfunction(state_dim, action_dim, n_levels).to(device)\n",
    "        self.freezing_q_function = Qfunction(state_dim, action_dim, n_levels).to(device)\n",
    "        self.freezing_q_function.load_state_dict(self.q_function.state_dict())\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = []\n",
    "        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "        self.counter = 1\n",
    "        self.period = period\n",
    "        self.N = 25_000\n",
    "        \n",
    "        self.n_levels = n_levels\n",
    "        self.taus = self.get_taus()\n",
    "\n",
    "    def get_taus(self):\n",
    "        taus = np.zeros((self.n_levels,))\n",
    "        for i in range(self.n_levels):\n",
    "            taus[i] = (i / self.n_levels + (i + 1) / self.n_levels) / 2.0\n",
    "        return taus\n",
    "\n",
    "    def get_action(self, obs, show=False):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(obs).unsqueeze(dim=0).to(device)\n",
    "            # print(f\"state: {state.shape}\")\n",
    "            action_reward = self.q_function(state)\n",
    "            action_reward = action_reward.squeeze().cpu().numpy()\n",
    "            q_values = np.sum(action_reward, axis=1) * 1.0 / self.n_levels\n",
    "            if show:\n",
    "                print(f\"state: {obs}\")\n",
    "                print(f\"q_value: {q_values}\")\n",
    "\n",
    "            max_action = np.argmax(q_values)\n",
    "                \n",
    "            probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "            probs[max_action] += 1.0 - self.epsilon\n",
    "            if show or self.counter % self.N == 0:\n",
    "                print(f\"probs: {probs}\")\n",
    "            action = np.random.choice(np.arange(self.action_dim), p=probs)\n",
    "            return action\n",
    "\n",
    "\n",
    "    def fit(self, state, action, reward, done, next_state, show=False):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > 10 * self.batch_size:\n",
    "\n",
    "            if self.counter % self.period == 0:\n",
    "                self.freezing_q_function.load_state_dict(self.q_function.state_dict())\n",
    "            \n",
    "            self.counter += 1\n",
    "\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, list(zip(*batch)))\n",
    "            states, actions, rewards, dones, next_states = states.to(device), actions.unsqueeze(1).to(device), rewards.unsqueeze(dim=1), dones.unsqueeze(dim=1) , next_states.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_action_rewards = self.q_function(next_states)\n",
    "                next_q_values = torch.sum(next_action_rewards, dim=2) * 1.0 / self.n_levels\n",
    "                max_next_action = torch.argmax(next_q_values, dim=1, keepdim=True)\n",
    "\n",
    "                max_next_action = max_next_action.expand((self.batch_size, self.n_levels)).unsqueeze(dim=1)\n",
    "\n",
    "                next_action_rewards = self.freezing_q_function(next_states)\n",
    "                # next_action_rewards: (batch_size, n_actions, n_atoms)\n",
    "                # max_next_action: (batch_size, 1, n_atoms) [[[2, 2, 2, 2 ...]], [[0, 0, 0, 0, ...]]]\n",
    "                reward_distrs = next_action_rewards.gather(1, max_next_action).squeeze()\n",
    "                  \n",
    "                if show or self.counter % self.N == 0:\n",
    "                    print(f\"shape next_action_rewards: {next_action_rewards.shape}\")\n",
    "                    print(f\"shape reward_distrs: {reward_distrs.shape}\")\n",
    "                    print(f\"next_q_values: {next_q_values}\")\n",
    "                    print(f\"max_next_action: {max_next_action}\")\n",
    "\n",
    "                target_distrs = rewards + self.gamma * (1 - dones) * reward_distrs\n",
    "            \n",
    "            # была ошибка здесь. Отдавал вместо actions - argmax_actions.\n",
    "            distrs = self.q_function(states.float())\n",
    "            actions = actions.long().expand((self.batch_size, self.n_levels)).unsqueeze(dim=1)\n",
    "            distrs = distrs.gather(1, actions).squeeze()\n",
    "\n",
    "            loss = None\n",
    "            for i in range(self.n_levels):\n",
    "                tau = self.taus[i]\n",
    "                ttau = torch.ones((self.batch_size, self.n_levels)) * tau\n",
    "                identity = (target_distrs > distrs[:, [i]]).long()\n",
    "                if i == 0:\n",
    "                    loss = torch.mean(torch.abs(identity - ttau) * torch.abs(target_distrs - distrs[:, [i]]))\n",
    "                else:\n",
    "                    loss += torch.mean(torch.abs(identity - ttau) * torch.abs(target_distrs - distrs[:, [i]]))\n",
    "                if show or self.counter % self.N == 0:\n",
    "                    print(f\"ttau: {ttau}\")\n",
    "                    print(f\"identity: {identity}\")\n",
    "                    print(f\"diff: {target_distrs - distrs[:, [i]]}\")\n",
    "\n",
    "            self.optimzaer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimzaer.step()\n",
    "\n",
    "            if show:\n",
    "                print(f\"loss: {loss}\")\n",
    "\n",
    "    def decrease_params(self):\n",
    "        self.epsilon = max(self.epsilon - self.epsilon_decrease, self.epsilon_min)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88162521-0874-4605-9538-0e6f1c8eabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_learning(env, agent, episode_n = 100, t_max = 500):\n",
    "    # agent.epsilon_decrease = 1.0 / (0.75*episode_n)\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episode_n):\n",
    "        total_reward = 0\n",
    "\n",
    "        show = False\n",
    "        if episode % 50 == 0:\n",
    "            show = False\n",
    "                \n",
    "        state, info = env.reset()\n",
    "        for t in range(t_max):\n",
    "                \n",
    "            action = agent.get_action(state, show)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "\n",
    "            if show:\n",
    "                print(f\"iteration: {t}\")\n",
    "            \n",
    "            agent.fit(state, action, reward, terminated or truncated, next_state, show)\n",
    "            \n",
    "    \n",
    "            state = next_state\n",
    "    \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(total_reward)\n",
    "        if episode % 10 == 0:\n",
    "            print(f'episode: {episode}, total_reward: {np.mean(total_rewards[-10:])}')\n",
    "            \n",
    "        agent.decrease_params()\n",
    "    \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae2b6b26-e7be-4ae7-8199-c4ecbfa058d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проблема при реализации была в том, что в модели добавились новые слои, а я эти слои не копировал, а копировал только общий позвоночник модели как в старой версии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d226d147-020c-4efa-82d5-01033ef86db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.81239525796218e-16"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d58f5d-03e4-404e-b8aa-47efaa3e4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import TransformReward\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = TransformReward(env, lambda r: v_min + r * (v_max - v_min) / 500.0)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "print(action_dim)\n",
    "\n",
    "episode_n=3000\n",
    "\n",
    "\n",
    "agent = DQN_double(state_dim, action_dim)\n",
    "\n",
    "total_rewards = DQN_learning(env, agent, episode_n=episode_n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
